[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Julius’s blog\nData analysis shenanigans mostly in R"
  },
  {
    "objectID": "posts/cherry_post/index.html",
    "href": "posts/cherry_post/index.html",
    "title": "Cherry Bloom Prediction",
    "section": "",
    "text": "Lets load all required packages.\n# Load all required packages\nlibrary(nasapower)\nlibrary(grateful)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(data.table)\nlibrary(missForest)\nlibrary(spls)\nlibrary(stringr)\nlibrary(mvgam)\nlibrary(tibble)"
  },
  {
    "objectID": "posts/cherry_post/index.html#cherry-blossom-day-prediction",
    "href": "posts/cherry_post/index.html#cherry-blossom-day-prediction",
    "title": "Cherry Bloom Prediction",
    "section": "Cherry Blossom day prediction",
    "text": "Cherry Blossom day prediction\nSo earlier this year I participated in International Cherry Blossom Prediction Competition, which invites you to predict this years bloom date of cherry trees in 5 different locations based on provided historical bloom dates provided and any data you dig out. Now that cherry trees bloomed long time ago, I am sharing my entry. As good ideas come after, I enhanced last modelling step by introducing State-space framework into GAM model, allowing to deal with observation poor time series in several of locations.\n\n1. Weather Data Import\nLet’s start by uploading historical weather data for studied locations. I found that NASA POWER project provides best continuous weather records using nasapower (Sparks 2018) package. Original records from nearby stations are riddled with missing data. So we download: Tmax, Tmin, Tmean and Precipitation from 1981-01-01 (earliest date available) until the most recent.\nHere’s how we fetch the data for Kyoto as an example:\n\n# Retrieve weather data using nasapower for Kyoto\nKyoto_temp &lt;- get_power(\n       community = \"ag\",\n       lonlat = c(135.6761, 35.0120),\n       pars = c(\"T2M\", \"T2M_MAX\",\"T2M_MIN\", \"PRECTOTCORR\"),\n       dates = c(\"1981-01-01\", \"2025-02-15\"),\n       temporal_api = \"daily\"\n)\n\nWe repeat this process for all five cherry blossom locations: Kyoto (Japan), Liestal-Weideli (Switzerland), Washington DC (USA), Vancouver (Canada), and New York City (USA).\n\n\n\n\n\n\nFull code for all locations\n\n\n\n\n\n\n# Retrieve weather data for remaining locations\nSwiss_temp &lt;- get_power(\n  community = \"ag\",\n  lonlat = c(7.730519, 47.4814),\n  pars = c(\"T2M\", \"T2M_MAX\",\"T2M_MIN\", \"PRECTOTCORR\"),\n  dates = c(\"1981-01-01\", \"2025-02-15\"),\n  temporal_api = \"daily\"\n)\n\nWashington_temp &lt;- get_power(\n  community = \"ag\",\n  lonlat = c(-77.0386, 38.8853),\n  pars = c(\"T2M\", \"T2M_MAX\",\"T2M_MIN\", \"PRECTOTCORR\"),\n  dates = c(\"1981-01-01\", \"2025-02-15\"),\n  temporal_api = \"daily\"\n)\n\nVancouver_temp &lt;- get_power(\n  community = \"ag\",\n  lonlat = c(-123.1636, 49.2237),\n   pars = c(\"T2M\", \"T2M_MAX\",\"T2M_MIN\", \"PRECTOTCORR\"),\n  dates = c(\"1981-01-01\", \"2025-02-15\"),\n  temporal_api = \"daily\"\n)\n\nNY_temp &lt;- get_power(\n  community = \"ag\",\n  lonlat = c(-73.99809, 40.73040),\n  pars = c(\"T2M\", \"T2M_MAX\",\"T2M_MIN\", \"PRECTOTCORR\"),\n  dates = c(\"1981-01-01\", \"2025-02-15\"),\n  temporal_api = \"daily\"\n)\n\n\n\n\n\n\n2. Calculating Daily Weather Anomalies\nNow let’s create daily anomalies. We assume that if weather conditions were identical, cherry blossoms would occur on the same day every year. By computing daily anomalies, we partially normalize our variables across different locations.\nFirst, we define a function to calculate these anomalies:\n\n# Function to calculate daily climate anomalies\ncalculate_climate_anomalies &lt;- function(data, \n                                        baseline_start = 1981, \n                                        baseline_end = 2024, \n                                        vars = c(\"T2M\", \"T2M_MAX\", \"T2M_MIN\", \"PRECTOTCORR\")) {\n  \n  # Validate input variables\n  if (!all(vars %in% names(data))) {\n    stop(\"Not all specified variables are present in the dataset.\")\n  }\n  if (!all(c(\"YEAR\", \"DOY\") %in% names(data))) {\n    stop(\"Dataset must include 'YEAR' and 'DOY' columns.\")\n  }\n  \n  # Create baseline subset\n  baseline_data &lt;- data[data$YEAR &gt;= baseline_start & data$YEAR &lt;= baseline_end, ]\n  \n  # Calculate daily climatology (mean for each day-of-year)\n  climatology &lt;- aggregate(baseline_data[vars], \n                           by = list(DOY = baseline_data$DOY), \n                           FUN = mean, \n                           na.rm = TRUE)\n  \n  # Merge climatology with original data and compute anomalies\n  result &lt;- merge(data, climatology, by = \"DOY\", suffixes = c(\"\", \"_mean\"))\n  for (var in vars) {\n    mean_col &lt;- paste0(var, \"_mean\")\n    anom_col &lt;- paste0(var, \"_anomaly\")\n    result[[anom_col]] &lt;- result[[var]] - result[[mean_col]]\n  }\n  \n  # Attach attributes and sort by date\n  attr(result, \"baseline_period\") &lt;- paste(baseline_start, \"-\", baseline_end)\n  attr(result, \"variables\") &lt;- vars\n  result &lt;- result[order(result$YYYYMMDD), ]\n  \n  return(result)\n}\n\nWe then apply this function to each location’s weather data. Here’s an example for New York:\n\n# Calculate anomalies for New York\nNY_temp_anomalies &lt;- calculate_climate_anomalies(NY_temp)\nNY_temp_anomalies$location &lt;- \"New York, USA\"\n\n\n\n\n\n\n\nProcessing all locations\n\n\n\n\n\n\n# Calculate anomalies for remaining locations\nVancouver_temp_anomalies &lt;- calculate_climate_anomalies(Vancouver_temp)\nSwiss_temp_anomalies &lt;- calculate_climate_anomalies(Swiss_temp)\nWashington_temp_anomalies &lt;- calculate_climate_anomalies(Washington_temp)\nKyoto_temp_anomalies &lt;- calculate_climate_anomalies(Kyoto_temp)\n\n# Append location identifiers\nVancouver_temp_anomalies$location &lt;- \"Vancouver, Canada\"\nSwiss_temp_anomalies$location &lt;- \"Liestal-Weideli, Switzerland\"\nWashington_temp_anomalies$location &lt;- \"Washington DC, USA\"\nKyoto_temp_anomalies$location &lt;- \"Kyoto, Japan\"\n\n\n\n\nFinally, we combine all the anomaly data into a single dataset:\n\n# Combine all anomaly data\ncombined_anomalies &lt;- rbind(\n  NY_temp_anomalies,\n  Vancouver_temp_anomalies,\n  Swiss_temp_anomalies,\n  Washington_temp_anomalies,\n  Kyoto_temp_anomalies\n)\ncombined_anomalies &lt;- combined_anomalies[order(combined_anomalies$location, combined_anomalies$YYYYMMDD), ]\n\n\n\n3. Importing and Processing Cherry Blossom Data\nNext, we import and combine the cherry blossom data. We filter for records from 1981 onward to ensure the dates match our weather data range:\n\n# Import cherry blossom data and filter for records from 1981 onward\nVancouver &lt;- read_csv(\"data/vancouver.csv\",show_col_types = FALSE)\nWashington &lt;- read_csv(\"data/washingtondc.csv\",show_col_types = FALSE)\nKyoto &lt;- read_csv(\"data/kyoto.csv\",show_col_types = FALSE)\nSwiss &lt;- read_csv(\"data/liestal.csv\",show_col_types = FALSE)\nNyc &lt;- read_csv(\"data/nyc.csv\",show_col_types = FALSE)\n\ncombined_blossom_dates &lt;- rbind(Vancouver, Washington, Kyoto, Swiss, Nyc)\ncombined_blossom_dates &lt;- combined_blossom_dates[order(combined_blossom_dates$location, combined_blossom_dates$year), ]\ncombined_blossom_dates &lt;- combined_blossom_dates[combined_blossom_dates$year &gt;= 1981, ]\n\n\n\n4. Augmenting New York Blossom Data\nThe primary dataset contains only one record for New York. To augment it, I retrieve data from the USA National Phenology Network. Based on recommendations, I filter for the specific location and species, then select records with more than 74% blossom (excluding the 50–74% category, which does not match our original data). For each year, I select the earliest record:\n\n# Read USA-NPN individual phenometrics data\nUSA_NPN_status_intensity &lt;- read_csv('data/USA-NPN_status_intensity_observations_data.csv', show_col_types = FALSE)\n\n# Convert to data frame and prepare Intensity_Value as factor\nUSA_status &lt;- as.data.frame(USA_NPN_status_intensity)\nUSA_status$Intensity_Value &lt;- as.factor(USA_status$Intensity_Value)\n\n# Filter out records with intensity values that do not match our criteria\nUSA_status_filtered &lt;- USA_status[!(USA_status$Intensity_Value %in% \n  c(\"-9999\", \"Little\", \"25-49%\", \"5-24%\", \"Less than 5%\", \"More than 10\", \"50-74%\")), ]\n\n# Convert dates and extract Year and Day of Year\nUSA_status_filtered$Date &lt;- mdy(USA_status_filtered$Observation_Date)\nUSA_status_filtered$Year &lt;- year(USA_status_filtered$Date)\nUSA_status_filtered$Day_of_Year &lt;- yday(USA_status_filtered$Date)\n\n\n\n\n\n\n\nVisualization and detailed summary\n\n\n\n\n\n\n# Create a scatter plot to visualize Day of Year vs. Year, colored by Intensity_Value\nggplot(USA_status_filtered, aes(x = Year, y = Day_of_Year, color = Intensity_Value)) +\n  geom_point(size = 3, alpha = 0.8) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Day of Year vs Year for USA-NPN Data\",\n       x = \"Year\",\n       y = \"Day of Year\",\n       color = \"Intensity Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Summarize the filtered data by year\nyear_summary_NY_individual &lt;- USA_status_filtered %&gt;%\n  group_by(Year) %&gt;%\n  summarize(\n    Min = min(Day_of_Year, na.rm = TRUE),\n    Q1 = quantile(Day_of_Year, 0.25, na.rm = TRUE),\n    Median = median(Day_of_Year, na.rm = TRUE),\n    Mean = mean(Day_of_Year, na.rm = TRUE),\n    Q3 = quantile(Day_of_Year, 0.75, na.rm = TRUE),\n    Max = max(Day_of_Year, na.rm = TRUE),\n    Count = n()\n  )\nprint(year_summary_NY_individual)\n\n# A tibble: 13 × 8\n    Year   Min    Q1 Median  Mean    Q3   Max Count\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1  2012    89  89      89    89    89     89     2\n 2  2013   104 110     142   137.  150    170     9\n 3  2014    98 108.    141   132.  156    160    11\n 4  2015   101 138     148   141.  153    156    24\n 5  2016   110 128.    135   134   143.   153     6\n 6  2017    87 110     138   128.  143    146     9\n 7  2018    79  99     101   103.  107    140    66\n 8  2019    96 101     106.  115.  124.   151    24\n 9  2020    78  90.5   100   107.  117.   243    32\n10  2021    86  99     106   111.  116    148    41\n11  2022    97 110.    122.  122.  136.   154    10\n12  2023    92 104     130.  130.  139.   255    18\n13  2024    88  94     106   110.  124    147    11\n\n\n\n\n\nNow we create augmented New York data using the earliest bloom date for each year:\n\n# Create new NY data frame from the yearly summary (excluding 2024)\nny_data &lt;- data.frame(\n  location = \"newyorkcity\",\n  lat = 40.73040,\n  long = -73.99809,\n  alt = 8.5,\n  year = year_summary_NY_individual$Year[year_summary_NY_individual$Year != 2024],\n  bloom_date = NA,\n  bloom_doy = year_summary_NY_individual$Min[year_summary_NY_individual$Year != 2024]\n)\n\n# Convert day-of-year to actual dates\nny_data$bloom_date &lt;- as.Date(ny_data$bloom_doy - 1, origin = paste0(ny_data$year, \"-01-01\"))\n\n# Merge with existing combined blossom dates\ncombined_blossom_dates &lt;- rbind(combined_blossom_dates, ny_data)\ncombined_blossom_dates &lt;- combined_blossom_dates[order(combined_blossom_dates$location, combined_blossom_dates$year), ]\n\n\n\n5. Analyzing Blossom Data\nFor effective modeling, we need a timeframe that aligns with the natural cherry blossom cycle rather than arbitrary calendar years. Let’s analyze our blossom date distribution to identify the optimal starting point for our “cherry year.”\n\n# Summarize blossom dates by location\nblossom_summary &lt;- combined_blossom_dates %&gt;%\n  group_by(location) %&gt;%\n  summarise(\n    Min = min(bloom_doy),\n    Q1 = quantile(bloom_doy, 0.25),\n    Median = median(bloom_doy),\n    Q3 = quantile(bloom_doy, 0.75),\n    Max = max(bloom_doy),\n    Range = max(bloom_doy) - min(bloom_doy)\n  )\nprint(blossom_summary)\n\n# A tibble: 5 × 7\n  location       Min    Q1 Median    Q3   Max Range\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 kyoto           84  93       95    99   109    25\n2 liestal         75  87       93   101   121    46\n3 newyorkcity     78  87       92    98   110    32\n4 vancouver       83  84.5     86    91    96    13\n5 washingtondc    74  86       91    95   101    27\n\n\nThe maximum third quartile (Q3) at day 100 represents when 75% of historical blooms have occurred in late–bloomer location—effectively marking the end of the bloom season across locations. While we could theoretically calculate predictors from one recorded bloom date to the next, this creates a circular dependency: to predict next year’s bloom date, we’d need to know it already to determine when to stop accumulating our weather predictors. By using day 100 as our universal cutoff, we establish a “cherry year” that runs from April 10 to April 9, nicely matching historical bloom cycles.\n\n\n6. Building Rolling Sum Predictors\nIn this section, we create predictors that capture the cumulative effect of weather anomalies over a 30-day period, which we hypothesize will influence the timing of cherry blossom.\nFirst, we convert to data.table and compute 30-day rolling sums for each anomaly type. Here’s an example for temperature anomalies:\n\n# Convert to data.table for efficient rolling calculations\ncombined_anomalies &lt;- as.data.table(combined_anomalies)\n\n# Example: Calculate 30-day rolling sums for average temperature anomalies\ncombined_anomalies[, temp_ave_pos_rollsum := frollsum(ifelse(T2M_anomaly &gt; 0, T2M_anomaly, 0), \n                                                      n = 30, align = \"right\"), by = location]\ncombined_anomalies[, temp_ave_neg_rollsum := frollsum(ifelse(T2M_anomaly &lt; 0, T2M_anomaly, 0), \n                                                      n = 30, align = \"right\"), by = location]\n\n\n\n\n\n\n\nComplete rolling sum calculations for all variables\n\n\n\n\n\n\n# Maximum temperature rolling sums\ncombined_anomalies[, temp_max_pos_rollsum := frollsum(ifelse(T2M_MAX_anomaly &gt; 0, T2M_MAX_anomaly, 0), \n                                                      n = 30, align = \"right\"), by = location]\ncombined_anomalies[, temp_max_neg_rollsum := frollsum(ifelse(T2M_MAX_anomaly &lt; 0, T2M_MAX_anomaly, 0), \n                                                      n = 30, align = \"right\"), by = location]\n\n# Minimum temperature rolling sums\ncombined_anomalies[, temp_min_pos_rollsum := frollsum(ifelse(T2M_MIN_anomaly &gt; 0, T2M_MIN_anomaly, 0), \n                                                      n = 30, align = \"right\"), by = location]\ncombined_anomalies[, temp_min_neg_rollsum := frollsum(ifelse(T2M_MIN_anomaly &lt; 0, T2M_MIN_anomaly, 0), \n                                                      n = 30, align = \"right\"), by = location]\n\n# Precipitation rolling sums\ncombined_anomalies[, prcp_pos_rollsum := frollsum(ifelse(PRECTOTCORR_anomaly &gt; 0, PRECTOTCORR_anomaly, 0), \n                                                  n = 30, align = \"right\"), by = location]\ncombined_anomalies[, prcp_neg_rollsum := frollsum(ifelse(PRECTOTCORR_anomaly &lt; 0, PRECTOTCORR_anomaly, 0), \n                                                  n = 30, align = \"right\"), by = location]\n\n\n\n\nNext, we create a “cherry year” that aligns with the bloom cycle. Based on our analysis, we start each cherry year on April 10 (day 100):\n\n# Create a numeric day-of-year and adjust to form a 'cherry_year'\ncombined_anomalies[, doy := as.numeric(format(YYYYMMDD, \"%j\"))]\ncombined_anomalies[, cherry_year := ifelse(doy &gt;= 100, year(YYYYMMDD) + 1, year(YYYYMMDD))]\ncombined_anomalies[, day_number := ifelse(doy &gt;= 100, doy - 99, doy + (366 - 99))]\n\nFinally, we create a streamlined dataset containing only the rolling sum predictors:\n\nsmall_anomaly_df &lt;- combined_anomalies %&gt;%\n  select(-c(DOY, LON, LAT, YEAR, MM, DD, T2M, T2M_MAX, T2M_MIN, PRECTOTCORR,\n            T2M_mean, T2M_MAX_mean, T2M_MIN_mean, PRECTOTCORR_mean,\n            T2M_anomaly, T2M_MAX_anomaly, T2M_MIN_anomaly, PRECTOTCORR_anomaly))\n\nThis process creates eight rolling sum predictors (positive and negative anomalies for average/max/min temperature and precipitation) that capture the cumulative weather effects leading up to each potential bloom date.\n\n# Load the pre-processed data since we skipped the processing steps\nfinal_wide &lt;- read.csv(\"data/final_wide.csv\", row.names = 1)\n\n\n\n7. Reshaping, Imputation, and Merging Wide-Format Data\nWe now convert our rolling sum predictors into a wide-format structure for modeling. This creates a matrix where each row represents a cherry year and location, with columns for each day’s rolling sum values.\nHere’s the process for one predictor (positive temperature anomaly rolling sum):\n\n# Reshape data to wide format\nwide_temp_ave_pos &lt;- dcast(small_anomaly_df[!is.na(temp_ave_pos_rollsum)], \n                           cherry_year + location ~ day_number, \n                           value.var = \"temp_ave_pos_rollsum\")\n\n# Rename columns to be more descriptive\nsetnames(wide_temp_ave_pos, \n         old = setdiff(names(wide_temp_ave_pos), c(\"cherry_year\", \"location\")), \n         new = paste0(\"temp_ave_pos_rollsum_\", seq_along(setdiff(names(wide_temp_ave_pos), c(\"cherry_year\", \"location\")))))\n\n# Impute missing values using random forest\nnumeric_cols &lt;- setdiff(names(wide_temp_ave_pos), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_temp_ave_pos[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_temp_ave_pos_imputed &lt;- cbind(wide_temp_ave_pos[, .(cherry_year, location)], imputed_matrix)\n\nWe repeat this process for all eight rolling sum predictors. Missing values (~2.4% per predictor) occur in three scenarios: - Initial period from April 10, 1981 to January 31, 1982 - Dates after February 20, 2025 (extent of available weather data) - Day 366 in non-leap years\n\n\n\n\n\n\nFull reshaping code for all predictors\n\n\n\n\n\n\n# Process negative temperature average anomalies\nwide_temp_ave_neg &lt;- dcast(small_anomaly_df[!is.na(temp_ave_neg_rollsum)], \n                           cherry_year + location ~ day_number, \n                           value.var = \"temp_ave_neg_rollsum\")\nsetnames(wide_temp_ave_neg, \n         old = setdiff(names(wide_temp_ave_neg), c(\"cherry_year\", \"location\")), \n         new = paste0(\"temp_ave_neg_rollsum_\", seq_along(setdiff(names(wide_temp_ave_neg), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_temp_ave_neg), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_temp_ave_neg[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_temp_ave_neg_imputed &lt;- cbind(wide_temp_ave_neg[, .(cherry_year, location)], imputed_matrix)\n\n# Process positive temperature maximum anomalies\nwide_temp_max_pos &lt;- dcast(small_anomaly_df[!is.na(temp_max_pos_rollsum)], \n                           cherry_year + location ~ day_number, \n                           value.var = \"temp_max_pos_rollsum\")\nsetnames(wide_temp_max_pos, \n         old = setdiff(names(wide_temp_max_pos), c(\"cherry_year\", \"location\")), \n         new = paste0(\"temp_max_pos_rollsum_\", seq_along(setdiff(names(wide_temp_max_pos), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_temp_max_pos), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_temp_max_pos[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_temp_max_pos_imputed &lt;- cbind(wide_temp_max_pos[, .(cherry_year, location)], imputed_matrix)\n\n# Process negative temperature maximum anomalies\nwide_temp_max_neg &lt;- dcast(small_anomaly_df[!is.na(temp_max_neg_rollsum)], \n                           cherry_year + location ~ day_number, \n                           value.var = \"temp_max_neg_rollsum\")\nsetnames(wide_temp_max_neg, \n         old = setdiff(names(wide_temp_max_neg), c(\"cherry_year\", \"location\")), \n         new = paste0(\"temp_max_neg_rollsum_\", seq_along(setdiff(names(wide_temp_max_neg), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_temp_max_neg), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_temp_max_neg[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_temp_max_neg_imputed &lt;- cbind(wide_temp_max_neg[, .(cherry_year, location)], imputed_matrix)\n\n# Process positive temperature minimum anomalies\nwide_temp_min_pos &lt;- dcast(small_anomaly_df[!is.na(temp_min_pos_rollsum)], \n                           cherry_year + location ~ day_number, \n                           value.var = \"temp_min_pos_rollsum\")\nsetnames(wide_temp_min_pos, \n         old = setdiff(names(wide_temp_min_pos), c(\"cherry_year\", \"location\")), \n         new = paste0(\"temp_min_pos_rollsum_\", seq_along(setdiff(names(wide_temp_min_pos), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_temp_min_pos), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_temp_min_pos[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_temp_min_pos_imputed &lt;- cbind(wide_temp_min_pos[, .(cherry_year, location)], imputed_matrix)\n\n# Process negative temperature minimum anomalies\nwide_temp_min_neg &lt;- dcast(small_anomaly_df[!is.na(temp_min_neg_rollsum)], \n                           cherry_year + location ~ day_number, \n                           value.var = \"temp_min_neg_rollsum\")\nsetnames(wide_temp_min_neg, \n         old = setdiff(names(wide_temp_min_neg), c(\"cherry_year\", \"location\")), \n         new = paste0(\"temp_min_neg_rollsum_\", seq_along(setdiff(names(wide_temp_min_neg), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_temp_min_neg), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_temp_min_neg[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_temp_min_neg_imputed &lt;- cbind(wide_temp_min_neg[, .(cherry_year, location)], imputed_matrix)\n\n# Process positive precipitation anomalies\nwide_prcp_pos &lt;- dcast(small_anomaly_df[!is.na(prcp_pos_rollsum)], \n                       cherry_year + location ~ day_number, \n                       value.var = \"prcp_pos_rollsum\")\nsetnames(wide_prcp_pos, \n         old = setdiff(names(wide_prcp_pos), c(\"cherry_year\", \"location\")), \n         new = paste0(\"prcp_pos_rollsum_\", seq_along(setdiff(names(wide_prcp_pos), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_prcp_pos), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_prcp_pos[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_prcp_pos_imputed &lt;- cbind(wide_prcp_pos[, .(cherry_year, location)], imputed_matrix)\n\n# Process negative precipitation anomalies\nwide_prcp_neg &lt;- dcast(small_anomaly_df[!is.na(prcp_neg_rollsum)], \n                       cherry_year + location ~ day_number, \n                       value.var = \"prcp_neg_rollsum\")\nsetnames(wide_prcp_neg, \n         old = setdiff(names(wide_prcp_neg), c(\"cherry_year\", \"location\")), \n         new = paste0(\"prcp_neg_rollsum_\", seq_along(setdiff(names(wide_prcp_neg), c(\"cherry_year\", \"location\")))))\nnumeric_cols &lt;- setdiff(names(wide_prcp_neg), c(\"cherry_year\", \"location\"))\nnumeric_matrix &lt;- as.matrix(wide_prcp_neg[, ..numeric_cols])\nimputed_result &lt;- missForest(numeric_matrix, maxiter = 5, verbose = TRUE)\nimputed_matrix &lt;- imputed_result$ximp\nwide_prcp_neg_imputed &lt;- cbind(wide_prcp_neg[, .(cherry_year, location)], imputed_matrix)\n\n\n\n\nAfter processing all eight predictor sets, we merge them into a comprehensive dataset:\n\n# Merge all wide-format datasets by cherry_year and location\nfinal_wide &lt;- Reduce(function(x, y) merge(x, y, by = c(\"cherry_year\", \"location\"), all = TRUE),\n                     list(wide_temp_ave_pos_imputed, wide_temp_ave_neg_imputed, \n                          wide_temp_max_pos_imputed, wide_temp_max_neg_imputed,\n                          wide_temp_min_pos_imputed, wide_temp_min_neg_imputed,\n                          wide_prcp_pos_imputed, wide_prcp_neg_imputed))\n\nThis gives us 8 rolling sum variables × 366 days = 2,928 predictors per location per year. Due to the computational intensity of the imputation process, you can download the pre-imputed file from final_wide.csv:\n\nwrite.csv(final_wide, \"data/final_wide.csv\", row.names = TRUE)\n\n\n\n8. Merging Rolling Sums with Blossom Data\nNow we merge our wide-format predictors with the actual bloom dates:\n\nfinal_wide &lt;- as_tibble(final_wide)\n\n# Map location names to match blossom data format\nlocation_mapping &lt;- c(\n  \"Kyoto, Japan\"                = \"kyoto\",\n  \"Liestal-Weideli, Switzerland\" = \"liestal\",\n  \"New York, USA\"               = \"newyorkcity\",\n  \"Vancouver, Canada\"           = \"vancouver\",\n  \"Washington DC, USA\"          = \"washingtondc\"\n)\nfinal_wide$location_mapped &lt;- location_mapping[final_wide$location]\n\n# Merge with bloom dates\nfinal_wide &lt;- merge(\n  final_wide,\n  combined_blossom_dates[, c(\"location\", \"year\", \"bloom_doy\")],\n  by.x = c(\"location_mapped\", \"cherry_year\"),\n  by.y = c(\"location\", \"year\"),\n  all.x = TRUE\n)\n\n# Reorder columns to put key variables first\nkeep_vars &lt;- c(\"cherry_year\", \"location\", \"bloom_doy\")\nexisting_keep_vars &lt;- keep_vars[keep_vars %in% colnames(final_wide)]\nother_vars &lt;- setdiff(names(final_wide), c(existing_keep_vars, \"location_mapped\"))\nfinal_wide &lt;- final_wide[, c(existing_keep_vars, other_vars)]\n\n\n\n9. Selecting and Engineering features for blossom date prediction using sparse Partial Least Squares\nIn this section, we use Sparse Partial Least Squares (SPLS) to identify key predictors and extract latent factors that summarize the high-dimensional weather data. We first convert our wide-format dataset into a matrix form and define our predictors and response (bloom day). Next, we perform 10-fold cross-validation to determine the optimal number of latent factors (K) and the sparsity level (eta). With these parameters, we fit the SPLS model to the training data, which selects a subset of predictors and computes a projection matrix. Finally, we use this projection to calculate latent factor scores (e.g., latent1, latent2, etc.) for all observations. These latent factors encapsulate the main variability in the predictors and are then used for further analysis and prediction of the bloom day. Currently we will ignore that this dataset comes from 5 different locations.\n\n# Prepare data for SPLS\nmy_data &lt;- as.data.frame(final_wide)\nid_cols &lt;- c(\"cherry_year\", \"location\", \"bloom_doy\")\n\npredictor_cols &lt;- setdiff(names(my_data), id_cols)\ntrain_data &lt;- subset(my_data, !is.na(bloom_doy))\n\nX_train &lt;- as.matrix(train_data[, predictor_cols])\nY_train &lt;- train_data$bloom_doy\n\n# Cross-validation for optimal parameters\nset.seed(123)\ncv.out &lt;- cv.spls(\n  X_train, \n  Y_train,\n  K   = 1:10,\n  eta = seq(0.1, 0.9, 0.1),\n  fold = 10\n)\n\neta = 0.1 \neta = 0.2 \neta = 0.3 \neta = 0.4 \neta = 0.5 \neta = 0.6 \neta = 0.7 \neta = 0.8 \neta = 0.9 \n\nOptimal parameters: eta = 0.3, K = 3\n\n\n\n\n\n\n\n\noptimal_K   &lt;- cv.out$K.opt\noptimal_eta &lt;- cv.out$eta.opt\n\n# Fit final SPLS model and extract latent factors\nfinal_model &lt;- spls(X_train, Y_train, K = optimal_K, eta = optimal_eta)\nX_all &lt;- as.matrix(my_data[, predictor_cols])\nX_all_std &lt;- sweep(X_all, 2, final_model$meanx, FUN = \"-\")\nX_all_std &lt;- sweep(X_all_std, 2, final_model$normx, FUN = \"/\")\nX_all_sub &lt;- X_all_std[, final_model$A, drop = FALSE]\nlatent_all &lt;- X_all_sub %*% final_model$projection\n\nnum_latent &lt;- ncol(latent_all)\ncolnames(latent_all) &lt;- paste0(\"latent\", seq_len(num_latent))\n\nlatent_df &lt;- data.frame(\n  cherry_year = my_data$cherry_year,\n  location    = my_data$location,\n  bloom_doy   = my_data$bloom_doy,\n  latent_all\n)\n\n\n\n10. Model Diagnostics and Latent Factor Interpretation\nIn this section, we first assess the performance of our Sparse PLS model using training data. We compute key statistics—such as R², RMSE, and MAE—to gauge how well the model explains the variance in the bloom day (with a higher R² indicating a better fit).\n\n# Predict on training data\ny_pred_train &lt;- predict(final_model, X_train)\nSST &lt;- sum((Y_train - mean(Y_train))^2)\nSSE &lt;- sum((Y_train - y_pred_train)^2)\nR2_train &lt;- 1 - SSE/SST\n\nMAE_train &lt;- mean(abs(Y_train - y_pred_train))\nRMSE_train &lt;- sqrt(mean((Y_train - y_pred_train)^2))\nn_selected_vars &lt;- length(final_model$A)\npercent_vars_selected &lt;- (n_selected_vars / length(predictor_cols)) * 100\n\n# Create diagnostic plot with statistics\nstats_text &lt;- paste0(\n  \"R² = \", round(R2_train, 3), \"\\n\",\n  \"RMSE = \", round(RMSE_train, 1), \" days\\n\",\n  \"MAE = \", round(MAE_train, 1), \" days\\n\",\n  \"Variables: \", n_selected_vars, \"/\", length(predictor_cols), \n  \" (\", round(percent_vars_selected, 0), \"%)\"\n)\n\nggplot(data.frame(Actual = Y_train, Predicted = y_pred_train), aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  annotate(\"text\", x = max(Y_train) - 5, y = min(y_pred_train) + 10, \n           label = stats_text, hjust = 1, vjust = 0, size = 4,\n           color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Predicted vs Actual Bloom Day\",\n       x = \"Actual Bloom DOY\", y = \"Predicted Bloom DOY\")\n\n\n\n\n\n\n\n\nNot so shabby! Now lets examine the latent factors:\n\n# Latent Factor Interpretation\n\n# Parse the projection matrix from the SPLS model\nproj_mat &lt;- final_model$projection\n# Each row name is in the form \"temp_ave_pos_rollsum_123\"\nparsed &lt;- str_match(rownames(proj_mat), \"^(.*)_(\\\\d+)$\")\ngroup_names &lt;- parsed[, 2]           # e.g., \"temp_ave_pos_rollsum\"\nday_indices &lt;- as.numeric(parsed[, 3])  # day number\n\n# Create a data frame of loadings with associated group and day\ndf_proj &lt;- as.data.frame(proj_mat)\ndf_proj$group &lt;- group_names\ndf_proj$day   &lt;- day_indices\n\n# Rename factor columns for clarity\nn_fac &lt;- ncol(proj_mat)\ncolnames(df_proj)[1:n_fac] &lt;- paste0(\"Factor\", seq_len(n_fac))\n\n# Pivot the data to long format: each row becomes (group, day, factor, loading)\ndf_long &lt;- pivot_longer(df_proj, \n                        cols = starts_with(\"Factor\"),\n                        names_to = \"factor\",\n                        values_to = \"loading\")\n\n# Build a complete grid for all groups, days (1:366), and factors\nall_groups &lt;- c(\"temp_ave_pos_rollsum\", \"temp_ave_neg_rollsum\", \n                \"temp_max_pos_rollsum\", \"temp_max_neg_rollsum\", \n                \"temp_min_pos_rollsum\", \"temp_min_neg_rollsum\", \n                \"prcp_pos_rollsum\", \"prcp_neg_rollsum\")\nall_factors &lt;- unique(df_long$factor)\ngrid_df &lt;- expand.grid(group = all_groups, day = 1:366, factor = all_factors, \n                       stringsAsFactors = FALSE)\n\n# Merge grid with loadings to include missing combinations as NA\ndf_plot &lt;- left_join(grid_df, df_long, by = c(\"group\", \"day\", \"factor\"))\n\n# Convert day index to a date for plotting; day 1 corresponds to April 10\nbase_date &lt;- as.Date(\"2020-04-10\")\ndf_plot$date &lt;- base_date + (df_plot$day - 1)\n\n# Extract variable type and anomaly direction from group name\ndf_plot &lt;- df_plot %&gt;%\n  mutate(\n    type = gsub(\"_pos_rollsum.*|_neg_rollsum.*\", \"\", group),\n    anomaly = ifelse(grepl(\"_pos_\", group), \"pos\", \"neg\")\n  ) %&gt;%\n  mutate(type = factor(type, levels = c(\"temp_min\", \"temp_ave\", \"temp_max\", \"prcp\")),\n         anomaly = factor(anomaly, levels = c(\"pos\", \"neg\")))\n\n# Heatmap of latent factor loadings\np_heatmap &lt;- ggplot(df_plot, aes(x = date, y = anomaly, fill = loading)) +\n  geom_tile() +\n  facet_grid(type ~ factor) +\n  scale_x_date(date_breaks = \"2 months\", date_labels = \"%b\") +  # Show every 2nd month\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0, \n                       na.value = \"grey90\",\n                       labels = scales::label_number(accuracy = 0.0001)) +\n  labs(title = \"Heatmap of SPLS Loadings by Factor and Variable Type\",\n       x = \"Months\", y = \"Anomaly (pos vs neg)\", fill = \"Loading\") +\n  theme_minimal()\n\nThese factors summarize the original rolling sum predictors, showing clear patterns. Negative temperature anomalies accumulated before the median bloom date load negatively; since these sums are negative, a larger accumulation of cold days actually delays blooming. In contrast, higher rolling sums of positive temperature anomalies—indicating warmer days—load negatively, which means that more warm days speed up the bloom. Additionally, the model captures a dormancy effect: a buildup of cold days in October and November tends to lead to an earlier bloom the following year. Similarly, lower-than-average precipitation starting in December is associated with earlier blooming. This interpretation confirms that the latent factors meaningfully reflect the influence of weather on cherry blossom timing.\nNow let’s examine how these latent factors cluster by location. Although our SPLS model was built on the combined dataset—ignoring that measurements come from five different sites with potentially different responses—we can still see some separation. For instance, the third latent factor reveals that different sites tend to occupy distinct regions in the factor space. This variability indicates that site-specific responses exist, which is why we ultimately use a generalized additive model (GAM) to flexibly capture these differences in our final analysis.\n\n# 1) Reshape data so each latent factor is in its own row\nlatent_long &lt;- latent_df %&gt;%\n  pivot_longer(\n    cols = starts_with(\"latent\"),  # or c(\"latent1\", \"latent2\", \"latent3\")\n    names_to = \"factor\",\n    values_to = \"score\"\n  )\n\n# 2) Create the scatter plot, facet by factor\np &lt;- ggplot(latent_long, aes(x = bloom_doy, y = score, color = location)) +\n  # Use na.rm=TRUE so rows with NA in bloom_doy or score are simply not plotted\n  geom_point(na.rm = TRUE) +\n  # Optional: add a smoothing trend line for each location\n  # geom_smooth(method = \"lm\", se = FALSE, na.rm = TRUE)\n\n  facet_wrap(~ factor, scales = \"free_y\") +\n  labs(\n    title = \"Bloom Day vs. Latent Factor Scores\",\n    x = \"Bloom Day of Year\",\n    y = \"Latent Factor Score\",\n    color = \"Location\"\n  ) +\n  theme_minimal()\n\nprint(p)\n\n\n\n\n\n\n\n\nAlthough our SPLS model was built on the combined dataset—ignoring that measurements come from five different sites with potentially different responses—we can still see some separation. For instance, the third latent factor reveals that different sites tend to occupy distinct regions in the factor space. This variability indicates that site-specific responses exist, which is why we ultimately use a generalized additive model (GAM) to flexibly capture these differences in our final analysis.\n\n\n11. Dynamic GAM Modeling for Forecasting\nDynamic GAM Modeling for Location-Specific Latent Factor Adaptation\nWe now have three powerful latent factors from our SPLS analysis that capture the main weather patterns driving cherry blossom variability. However, these factors were derived as linear predictors from data pooled across all five locations, assuming each site responds identically to the same weather patterns.\nThe problem: Real cherry trees don’t follow universal rules. Each location likely has its own unique sensitivity to weather - Kyoto’s cherry blossoms might respond differently to warming than Vancouver’s, and some locations have frustratingly sparse observations that make traditional modeling challenging.\nThe approach: So I decided to go with mvgam (Clark and Wells 2023). mvgam stands for MultiVariate (Dynamic) Generalized Additive Models and is specifically designed for time series analysis like ours:\n\nTime series focus: Specifically designed for time series prediction with built-in capabilities to model autocorrelation if needed.\nGAM flexibility: At its core, it’s still a GAM, making it ideal for bending our otherwise linear latent factors to each location individually.\nState-space robustness: The state-space modeling framework is perfect for time series with missing observations. It models the underlying bloom process separately from the observations, so our sparse New York and Vancouver data doesn’t break the model—missing data areas handled by modeling the latent bloom process as it evolves through time.\nBayesian uncertainty: The Bayesian framework provides natural uncertainty quantification for both parameters and predictions.\n\nData preparation and model setup\nFirst, I split my data into training and testing sets at 2024 to ensure at least one bloom date for each location in our forecast evaluation. For state-space modeling, I need both series (to identify different time series) and trend (for the process formula) variables.\nMy model structure uses shrinkage smooths (bs=\"sz\") to handle the location-specific responses. For example, s(latent1, k=3) creates the main smooth effect of latent1, while s(trend, latent1, bs=\"sz\", k=3) adds location-specific deviations. The “sz” basis applies shrinkage - locations with sparse data have their specific effects shrunk toward zero, essentially borrowing the response pattern from data-rich locations. Meanwhile, Kyoto and others with complete records can maintain their unique response curves.\nLet’s prepare the data and fit the model:\n\ndf &lt;- latent_df\ndf$location &lt;- as.factor(df$location)\ndf$series &lt;- df$location  # Keep series for identifying time series\ndf$trend &lt;- df$location   # Add trend for trend_formula\ndf$time &lt;- df$cherry_year - 1981 #For numerical stability\n\n# Split data into training (&lt;2024) and testing (&gt;=2024)\ntrain_mvgam &lt;- df[df$cherry_year &lt; 2024, ]\ntest_mvgam  &lt;- df[df$cherry_year &gt;= 2024, ]\n\n\n# Now fit with tweaked priors\nmodel_gaussian &lt;- mvgam(formula = bloom_doy ~ series,\n                     trend_formula = ~ s(latent1, k=3) + s(trend, latent1, bs=\"sz\",k=3) +\n                           s(latent2, k=3) + s(trend, latent2, bs=\"sz\",k=3) +\n                           s(latent3, k=3) + s(trend, latent3, bs=\"sz\",k=3),\n                     data = train_mvgam,\n                     newdata = test_mvgam,\n                     noncentred = TRUE,\n                     family = gaussian(),silent = 2)\n\nLet’s examine the model diagnostics:\n\nsummary(model_gaussian, include_betas = FALSE)\n\nGAM observation formula:\nbloom_doy ~ series\n\nGAM process formula:\n~s(latent1, k = 3) + s(trend, latent1, bs = \"sz\", k = 3) + s(latent2, \n    k = 3) + s(trend, latent2, bs = \"sz\", k = 3) + s(latent3, \n    k = 3) + s(trend, latent3, bs = \"sz\", k = 3)\n\nFamily:\ngaussian\n\nLink function:\nidentity\n\nTrend model:\nNone\n\nN process models:\n5 \n\nN series:\n5 \n\nN timepoints:\n45 \n\nStatus:\nFitted using Stan \n4 chains, each with iter = 1000; warmup = 500; thin = 1 \nTotal post-warmup draws = 2000\n\n\nObservation error parameter estimates:\n             2.5%  50% 97.5% Rhat n_eff\nsigma_obs[1] 0.91 2.30   3.0 1.05    61\nsigma_obs[2] 2.30 3.50   4.7 1.01   350\nsigma_obs[3] 5.70 8.30  14.0 1.00  1538\nsigma_obs[4] 0.15 0.95   5.7 1.01   591\nsigma_obs[5] 1.10 3.40   4.4 1.09    45\n\nGAM observation model coefficient (beta) estimates:\n                                   2.5%   50% 97.5% Rhat n_eff\n(Intercept)                        88.0 94.00  99.0    1  1148\nseriesLiestal-Weideli, Switzerland -4.8 -1.70   1.1    1   956\nseriesNew York, USA                -2.6  0.81   4.9    1  1569\nseriesVancouver, Canada            -4.4 -1.40   1.7    1   844\nseriesWashington DC, USA           -5.5 -1.70   2.0    1  1339\n\nProcess error parameter estimates:\n           2.5%  50% 97.5% Rhat n_eff\nsigma[1] 0.0160 0.38   2.1 1.05    71\nsigma[2] 0.0180 0.41   2.4 1.01   292\nsigma[3] 0.0092 0.34   2.0 1.00  2775\nsigma[4] 0.0160 0.48   2.1 1.00  1250\nsigma[5] 0.0140 0.43   3.2 1.11    40\n\nGAM process model coefficient (beta) estimates:\n                  2.5%   50% 97.5% Rhat n_eff\n(Intercept)_trend -5.1 0.076   5.5    1  1396\n\nApproximate significance of GAM process smooths:\n                   edf Ref.df  Chi.sq p-value    \ns(latent1)        1.01      2 4706.30 &lt; 2e-16 ***\ns(series,latent1) 7.61     12  128.89    0.36    \ns(latent2)        1.02      2  638.76 &lt; 2e-16 ***\ns(series,latent2) 3.31     12    1.97    1.00    \ns(latent3)        1.01      2  291.30 1.7e-05 ***\ns(series,latent3) 1.59     12    1.37    1.00    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStan MCMC diagnostics:\n✔ No issues with effective samples per iteration\n✖ Rhats above 1.05 found for some parameters\n    Use pairs() and mcmc_plot() to investigate\n✖ 105 of 2000 iterations ended with a divergence (5.25%)\n    Try a larger adapt_delta to remove divergences\n✔ No issues with maximum tree depth\n\nSamples were drawn using sampling(hmc). For each parameter, n_eff is a\n  crude measure of effective sample size, and Rhat is the potential scale\n  reduction factor on split MCMC chains (at convergence, Rhat = 1)\n\nUse how_to_cite() to get started describing this model\n\n\nThe model shows severe convergence issues. Several variance parameters (sigmas) from both the observation and process models are poorly identified, with terrible effective sample sizes and Rhat values. Let me unpack what’s happening here. In our state-space formulation:\nThe baseline: Without weather effects, each location would bloom on roughly the same day every year (captured by the location intercepts) The process model: Our SPLS-derived latent factors predict deviations from this baseline. The process error represents additional year-to-year variation not captured by weather - perhaps soil conditions, tree age, or other unmeasured factors The observation model: This is where bloom dates actually get recorded. The observation error represents how much our weather-based predictions miss the true bloom dates\nThe identifiability crisis occurs because the model can explain the same bloom variability in multiple ways. High process error + low observation error? Low process error + high observation error? Without constraints, the model can’t decide. Here’s the key insight: Our SPLS model achieved good R² and MAE when fitted on pooled data from all locations. This means it captures the “average” weather-bloom relationship well, but likely misses location-specific nuances by roughly the same amount everywhere. The observation error is essentially quantifying “how wrong is our one-size-fits-all weather model?” By setting share_obs_params = TRUE, I enforce this logic: all locations share the same observation variance because they all use the same pooled SPLS model. This constraint breaks the identifiability problem while still allowing location-specific process errors to capture unique temporal patterns. Additionally, with 19% of iterations ending in divergences, I’ll increase adapt_delta to 0.99, max_treedepth to 12, and number of iterations for more careful exploration of the posterior:\n\nmodel_shared_sz &lt;- mvgam(formula = bloom_doy ~ series,\n                       trend_formula = ~ s(latent1, k=3) + s(trend, latent1, bs=\"sz\",k=3) +\n                           s(latent2, k=3) + s(trend, latent2, bs=\"sz\",k=3) +\n                           s(latent3, k=3) + s(trend, latent3, bs=\"sz\",k=3),\n                     data = train_mvgam,\n                     newdata = test_mvgam,\n                     share_obs_params = TRUE,\n                     noncentred = TRUE,\n                     control = list(adapt_delta = 0.99, max_treedepth = 12),\n                     burnin = 500,\n                     samples = 2000,\n                     family = gaussian(),silent = 2)\n\nsummary(model_shared_sz, include_betas = FALSE)\n\nGAM observation formula:\nbloom_doy ~ series\n\nGAM process formula:\n~s(latent1, k = 3) + s(trend, latent1, bs = \"sz\", k = 3) + s(latent2, \n    k = 3) + s(trend, latent2, bs = \"sz\", k = 3) + s(latent3, \n    k = 3) + s(trend, latent3, bs = \"sz\", k = 3)\n\nFamily:\ngaussian\n\nLink function:\nidentity\n\nTrend model:\nNone\n\nN process models:\n5 \n\nN series:\n5 \n\nN timepoints:\n45 \n\nStatus:\nFitted using Stan \n4 chains, each with iter = 2500; warmup = 500; thin = 1 \nTotal post-warmup draws = 8000\n\n\nObservation error parameter estimates:\n          2.5% 50% 97.5% Rhat n_eff\nsigma_obs  2.7 3.2   3.8    1   846\n\nGAM observation model coefficient (beta) estimates:\n                                   2.5%   50% 97.5% Rhat n_eff\n(Intercept)                        88.0 94.00  99.0    1  2749\nseriesLiestal-Weideli, Switzerland -5.1 -1.60   1.5    1  3642\nseriesNew York, USA                -2.5  0.74   4.3    1  3997\nseriesVancouver, Canada            -4.8 -0.90   2.5    1  6999\nseriesWashington DC, USA           -5.8 -1.30   2.7    1  3233\n\nProcess error parameter estimates:\n           2.5%  50% 97.5% Rhat n_eff\nsigma[1] 0.0098 0.25   1.1    1  7133\nsigma[2] 0.0170 0.43   2.1    1  1938\nsigma[3] 0.5900 4.40   6.7    1   424\nsigma[4] 0.0120 0.34   1.8    1 12251\nsigma[5] 0.0130 0.41   2.0    1  2242\n\nGAM process model coefficient (beta) estimates:\n                  2.5%   50% 97.5% Rhat n_eff\n(Intercept)_trend -5.4 0.045   5.2    1  3020\n\nApproximate significance of GAM process smooths:\n                    edf Ref.df  Chi.sq p-value    \ns(latent1)        1.009      2 4027.93 &lt; 2e-16 ***\ns(series,latent1) 7.792     12  233.66    0.12    \ns(latent2)        1.082      2  635.27 &lt; 2e-16 ***\ns(series,latent2) 3.099     12    2.42    1.00    \ns(latent3)        0.999      2  296.04 3.4e-05 ***\ns(series,latent3) 2.233     12    1.09    1.00    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nStan MCMC diagnostics:\n✔ No issues with effective samples per iteration\n✔ Rhat looks good for all parameters\n✔ No issues with divergences\n✔ No issues with maximum tree depth\n\nSamples were drawn using sampling(hmc). For each parameter, n_eff is a\n  crude measure of effective sample size, and Rhat is the potential scale\n  reduction factor on split MCMC chains (at convergence, Rhat = 1)\n\nUse how_to_cite() to get started describing this model\n\ngratia::draw(model_shared_sz, trend_effects=TRUE)\n\n\n\n\n\n\n\n\nThe enhanced sampling and shared observation parameters completely fixed our convergence issues. All parameters now have excellent effective sample sizes and Rhat values of 1.00. The shared observation error settled at about 3.2 days (pretty much as MAE of joint SPLS model) - this represents how much our weather-based predictions typically miss the actual bloom dates. Looking at the smooth effects, the main patterns are beautifully linear, exactly as expected from our SPLS-derived factors. Each latent factor shows a strong linear relationship with bloom timing (all p &lt; 0.001). The location-specific deviations (s(series,latent) terms) aren’t statistically significant, which isn’t surprising given our limited data. However, they still capture subtle location-specific responses that contribute to more accurate predictions.\nSo lets now run predictions for 2025 and compare with real data.\n\n# Forecast\nfc &lt;- forecast(model_shared_sz, test_mvgam)\n\n# Map full location names to short names for predictions\nlocation_mapping &lt;- c(\n  \"Kyoto, Japan\"                = \"kyoto\",\n  \"Liestal-Weideli, Switzerland\" = \"liestal\",\n  \"New York, USA\"               = \"newyorkcity\",\n  \"Vancouver, Canada\"           = \"vancouver\",\n  \"Washington DC, USA\"          = \"washingtondc\"\n)\n\n# Create dataframe with actual 2025 bloom dates\nactual_2025 &lt;- data.frame(\n  location = c(\"liestal\", \"vancouver\", \"kyoto\", \"washingtondc\", \"newyorkcity\"),\n  actual_date = as.Date(c(\"2025-03-27\", \"2025-04-03\", \"2025-04-04\", \"2025-03-28\", \"2025-04-04\")),\n  actual_doy = c(86, 93, 94, 87, 94),\n  stringsAsFactors = FALSE\n)\n\n# Extract posterior draws for all locations\nposterior_draws &lt;- list()\nfor (i in seq_along(fc$series_names)) {\n  full_name &lt;- as.character(fc$series_names[i])\n  short_name &lt;- location_mapping[full_name]\n  posterior_draws[[short_name]] &lt;- fc$forecasts[[i]][, 2]  # 2025 predictions\n}\n\n# Convert to long format for plotting\ndraws_df &lt;- data.frame(\n  value = unlist(posterior_draws),\n  location = rep(names(posterior_draws), each = length(posterior_draws[[1]]))\n)\n\n# Add actual values\ndraws_df &lt;- merge(draws_df, actual_2025[, c(\"location\", \"actual_doy\")], by = \"location\", all.x = TRUE)\n\n# Add proper location names\ndraws_df$location_full &lt;- factor(draws_df$location, \n  levels = c(\"kyoto\", \"liestal\", \"newyorkcity\", \"vancouver\", \"washingtondc\"),\n  labels = c(\"Kyoto, Japan\", \n             \"Liestal, Switzerland\", \n             \"New York City, USA\", \n             \"Vancouver, Canada\", \n             \"Washington DC, USA\"))\n\n# Calculate summary statistics with rounded differences\nsummary_stats &lt;- draws_df %&gt;%\n  group_by(location_full, actual_doy) %&gt;%\n  summarise(\n    median_pred = median(value),\n    mean_pred = mean(value),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    diff = round(median_pred - actual_doy, 0),\n    diff_text = case_when(\n      diff &gt; 0 ~ paste0(\"+\", diff, \" days\"),\n      diff &lt; 0 ~ paste0(diff, \" days\"),\n      TRUE ~ \"Perfect!\"\n    )\n  )\n\n# Create the plot\np_presentation &lt;- ggplot() +\n  # Density plots\n  geom_density(data = draws_df, \n               aes(x = value, fill = location_full), \n               alpha = 0.8,\n               color = \"white\",\n               linewidth = 0.5) +\n  \n  # Actual date lines\n  geom_segment(data = summary_stats,\n               aes(x = actual_doy, xend = actual_doy, y = 0, yend = Inf),\n               color = \"black\", \n               linewidth = 1.5,\n               linetype = \"solid\") +\n  \n  # Error labels\n  geom_label(data = summary_stats,\n             aes(x = actual_doy + 2, y = Inf, \n                 label = ifelse(diff == 0, \"Spot on!\", diff_text)),\n             vjust = 1.5,\n             size = 3,\n             fontface = \"bold\",\n             fill = \"white\",\n             alpha = 0.9,\n             label.padding = unit(0.25, \"lines\"),\n             label.size = 0.3) +\n  \n  facet_wrap(~ location_full, scales = \"free_y\", nrow = 1) +\n  scale_fill_manual(values = c(\"#2E7D32\", \"#1976D2\", \"#D32F2F\", \"#7B1FA2\", \"#F57C00\")) +\n  coord_cartesian(xlim = c(78, 112)) +  # This prevents cutting off distributions\n  scale_x_continuous(breaks = seq(80, 110, by = 10)) +\n  \n  labs(title = \"2025 Cherry Blossom Predictions vs Reality\",\n       x = \"Day of Year (vertical line = actual bloom date)\",\n       y = \"\") +\n  \n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    strip.text = element_text(size = 11, face = \"bold\"),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 9),\n    panel.grid = element_blank(),\n    panel.spacing = unit(0.5, \"lines\"),\n    plot.margin = margin(15, 15, 15, 15)\n  )\n\n# Display the plot\nprint(p_presentation)\n\n\n\n\n\n\n\n\nTwo perfect predictions (Kyoto and Vancouver), and the rest within a week - not bad for a model predicting a complex biological phenomenon months in advance. What started as a challenge to predict cherry blossoms using weather anomalies has demonstrated the power of combining dimensional reduction (SPLS) with flexible modeling (mvgam). The approach successfully handled the twin challenges of high-dimensional weather data and sparse observations, delivering predictions with well-calibrated uncertainty. Most satisfying is that the locations with the least data (New York and Vancouver) performed just as well as data-rich sites - proof that our shrinkage approach effectively borrowed strength across locations. Cherry trees may not follow universal rules, but with the right modeling framework, we can still capture their unique responses to weather patterns\n\n\nWe used R version 4.5.1 (R Core Team 2025) and the following R packages: data.table v. 1.17.6 (Barrett et al. 2025), gratia v. 0.10.0 (Simpson 2024), missForest v. 1.5 (Stekhoven and Buehlmann 2012; Stekhoven 2022), mvgam v. 1.1.51 (Clark and Wells 2023), nasapower v. 4.2.5 (Sparks 2018), rmarkdown v. 2.29 (Xie, Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020; Allaire et al. 2024), scales v. 1.4.0 (Wickham, Pedersen, and Seidel 2025), spls v. 2.2.3 (Chung, Chun, and Keles 2019), tidyverse v. 2.0.0 (Wickham et al. 2019)."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Swimming speed Analysis With Generalized Additive Models",
    "section": "",
    "text": "As a swimmer seeking to enhance performance, tracking workout data can offer valuable insights into progress over time. Analysis options for swimming compared to running in Garmin are instead week. I enjoy swimming and data analysis. In this analysis, I’ll explore several years of swimming data extracted from Garmin FIT files to figure out if I am getting faster."
  },
  {
    "objectID": "posts/post-with-code/index.html#data-extraction-from-fit-files",
    "href": "posts/post-with-code/index.html#data-extraction-from-fit-files",
    "title": "Swimming speed Analysis With Generalized Additive Models",
    "section": "Data Extraction from FIT Files",
    "text": "Data Extraction from FIT Files\nFirst, I’ll extract swimming data from all available FIT files. The FITfileR (Smith 2025) package allows us to read Garmin workout files and identify pool swimming sessions.\n\nlibrary(FITfileR)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(utils)\nlibrary(ggplot2)\nlibrary(hexbin)\nlibrary(viridis)\nlibrary(gratia)\nlibrary(mgcv)\nlibrary(tidyr)\nlibrary(performance)\nlibrary(lubridate)\n\n\nIdentifying Pool Swimming Sessions\nSo, one can ask Garmin to download all the files. Swim session or activity files are stored in .fit format and located in the DI_CONNECT folder, specifically in the subfolder DI-Connect-Uploaded-Files. File names are rather cryptical, so I first look for files that have “lap_swimming” attribute within.\n\n# Scan directory for all .fit files\nall_files &lt;- list.files(\"/Users/jb/Documents/BW_analysis/Swim\",\n                        pattern    = \"\\\\.fit$\",\n                        full.names = TRUE)\n\n# Check each file to identify pool swimming sessions\nn  &lt;- length(all_files)\nok &lt;- logical(n)\npb &lt;- txtProgressBar(min = 0, max = n, style = 3)\n\nfor (i in seq_along(all_files)) {\n  path &lt;- all_files[i]\n  # Safely check if file contains pool swimming data\n  ok[i] &lt;- possibly(function(p) {\n    sp &lt;- getMessagesByType(readFitFile(p), \"sport\")\n    isTRUE(any(sp$sport == \"swimming\" & sp$sub_sport == \"lap_swimming\",\n               na.rm = TRUE))\n  }, otherwise = FALSE)(path)\n  setTxtProgressBar(pb, i)\n}\n\nclose(pb)\n\npool_lap_fits &lt;- all_files[ok]\ncat(\"\\nFound\", length(pool_lap_fits), \"pool-lap swim files\\n\")\n\n\n\nExtracting Session Information\nNow, by examining several lap_swimming attributes containing files, I found that pool length in meters and activity data are stored in session, and length contains the workout information. \n\nsessions &lt;- map_dfr(pool_lap_fits, function(f) {\n  sess &lt;- tryCatch(getMessagesByType(readFitFile(f), \"session\"),\n                   error = function(e) NULL)\n  if (is.null(sess)) return(NULL)\n  tibble(\n    file          = f,\n    session_date  = as.Date(sess$start_time),\n    pool_length   = sess$pool_length\n  )\n}) %&gt;%\n  group_by(session_date) %&gt;%\n  mutate(session_number = row_number()) %&gt;%\n  ungroup()\n\n# Extract individual length (lap) data\nraw_laps &lt;- map_dfr(pool_lap_fits, function(f) {\n  lengths &lt;- tryCatch(getMessagesByType(readFitFile(f), \"length\"),\n                      error = function(e) NULL)\n  if (is.null(lengths)) return(NULL)\n  lengths %&gt;% mutate(file = f)\n})"
  },
  {
    "objectID": "posts/post-with-code/index.html#data-processing-and-set-detection",
    "href": "posts/post-with-code/index.html#data-processing-and-set-detection",
    "title": "Swimming speed Analysis With Generalized Additive Models",
    "section": "Data Processing and Set Detection",
    "text": "Data Processing and Set Detection\nSwimming workouts typically consist of multiple sets with rest periods between each set. I’ll identify these sets and calculate the relative positions of each lap within the workout and set itself:\n\n#Identify sets based on idle periods\nlaps_with_sets &lt;- raw_laps %&gt;%\n  group_by(file) %&gt;%\n  mutate(\n    lap_index          = row_number(),\n    total_laps         = n(),\n    pos_within_workout = lap_index / total_laps,\n    # New set starts after each idle period\n    set_id             = cumsum(length_type == \"idle\") + 1\n  ) %&gt;%\n  ungroup()\n\n# Process active laps only and calculate set-specific metrics\nfinal_tbl &lt;- laps_with_sets %&gt;%\n  filter(length_type == \"active\", swim_stroke != \"drill\") %&gt;%\n  group_by(file, set_id) %&gt;%\n  mutate(\n    lap_in_set     = row_number(),\n    set_size       = n(),\n    pos_within_set = lap_in_set / set_size\n  ) %&gt;%\n  ungroup() %&gt;%\n  left_join(sessions, by = \"file\") %&gt;%\n  select(\n    session_date,\n    session_number,\n    pool_length,\n    total_elapsed_time,\n    set_id,\n    lap_index,\n    lap_in_set,\n    pos_within_workout,\n    pos_within_set,\n    swim_stroke\n  )\n\n# Save processed data\nwrite.csv(final_tbl, \"all_swim_laps.csv\", row.names = FALSE)\n\nYou can load it directly in R:\n\nfinal_tbl &lt;- read.csv(\"https://jbogomolovas2.github.io/julius-blog/posts/post-with-code/all_swim_laps.csv\")\n\nI’ll identify gaps in training and analyze data from July 2020 onward to have a nice, recent, and consistent training segment to analyze.\n\n# Identify training gaps\nsessions_df &lt;- final_tbl %&gt;%\n  distinct(session_date) %&gt;%\n  arrange(session_date)\n\ngaps_df &lt;- sessions_df %&gt;%\n  # 1) make sure session_date is a Date\n  mutate(session_date = as.Date(session_date)) %&gt;%\n  # 2) grab the “previous” date\n  mutate(prev_date = lag(session_date)) %&gt;%\n  # 3) compute gap in days\n  mutate(gap_days = as.numeric(difftime(session_date, prev_date, units = \"days\"))) %&gt;%\n  # 4) drop the first row (where lag was NA)\n  filter(!is.na(gap_days))\n\n# Show top 5 largest training gaps\ntop5_gaps &lt;- gaps_df %&gt;%\n  arrange(desc(gap_days)) %&gt;%\n  slice(1:5) %&gt;%\n  rename(\n    end_date   = session_date,\n    start_date = prev_date\n  )\n\nprint(top5_gaps)\n\nLet’s perform some data cleaning: select a segment, calculate swim speeds, remove super-fast laps (I wish they were real!), and create a simple time variable.\n\nfinal_tbl_2020 &lt;- final_tbl %&gt;%\n  # 1) ensure session_date is Date\n  mutate(session_date = as.Date(session_date)) %&gt;%\n  \n  # 2) filter from July 7, 2020 onward\n  filter(session_date &gt;= ymd(\"2020-07-07\")) %&gt;%\n  \n  # 3) compute speed and factor columns\n  mutate(\n    speed       = pool_length / total_elapsed_time,\n    swim_stroke = factor(swim_stroke),\n    session_id  = factor(session_date)   # or combine with session_number if you need uniqueness\n  ) %&gt;%\n  \n  # 4) remove unrealistically fast laps\n  filter(speed &lt; 2.5) %&gt;%\n  \n  # 5) sort and compute days since first session\n  arrange(session_date) %&gt;%\n  mutate(\n    days_since_start = as.integer(\n      difftime(session_date,\n               min(session_date),\n               units = \"days\")\n    )\n  )"
  },
  {
    "objectID": "posts/post-with-code/index.html#visualizing-speed-distributions",
    "href": "posts/post-with-code/index.html#visualizing-speed-distributions",
    "title": "Swimming speed Analysis With Generalized Additive Models",
    "section": "Visualizing Speed Distributions",
    "text": "Visualizing Speed Distributions\nLet’s examine how swimming speed varies over time for different strokes. As I have a bunch of laps, let’s plot them as densities.\n\np_contour &lt;- ggplot() +\n  geom_point(data = final_tbl_2020, \n             aes(x = session_date, y = speed),\n             alpha = 0.05, size = 0.5, color = \"grey50\") +\n  geom_density_2d(data = final_tbl_2020, \n                  aes(x = session_date, y = speed),\n                  color = \"darkblue\", bins = 10, linewidth = 0.5) +\n  facet_wrap(~ swim_stroke, scales = \"free_y\", ncol = 2) +\n  labs(x = \"Session Date\",\n       y = \"Speed (m/s)\",\n       title = \"Swimming Speed Over Time by Stroke Type\",\n       subtitle = \"Density contours showing speed distribution\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.grid.minor = element_blank(),\n        strip.text = element_text(size = 12, face = \"bold\"))\n\nprint(p_contour)"
  },
  {
    "objectID": "posts/post-with-code/index.html#statistical-modeling-with-gams",
    "href": "posts/post-with-code/index.html#statistical-modeling-with-gams",
    "title": "Swimming speed Analysis With Generalized Additive Models",
    "section": "Statistical Modeling with GAMs",
    "text": "Statistical Modeling with GAMs\nI have something that is more or less monomodal. Therefore, my average swim speed would be a good indicator of my progress. However, in workouts, I do different sets, such as sprints, cooldowns, etc. Which vary in intensity, length, and position within the workout. I hypothesize that these factors significantly affect “average” swim speed and need to be accounted for. I don’t expect them to be linearly related, so I enter the magical world of Generalized Additive Models (GAMs). I will be using mgcv (Wood 2011) for fitting, gratia (Simpson 2024) for visualization, and performance (Lüdecke et al. 2021) for model assessment. I will start with a simple speed ~ time model and try to add covariates to get a better fit, closer to “average” swim speed. As I am working with a rather large dataset, I will use the bam function and a few corresponding tweaks: discrete = TRUE and method = \"fREML\" which enables faster computation for extensive datasets. By examining the swimming speed distribution, I clearly see some tails, so I will use the scaled t-distribution family = scat(). \n\n# Model 1: Simple time trend by stroke\nsimple &lt;- bam(\n    speed ~ swim_stroke + pool_length + \n            s(days_since_start, by = swim_stroke, bs = \"gp\"),\n    data     = final_tbl_2020,\n    discrete = TRUE,\n    family   = scat(),  # Scaled t-distribution for robustness\n    method   = \"fREML\"\n)\n\n# Model 2: Add position effects (fatigue within workout/set)\nset_location &lt;- bam(\n    speed ~ swim_stroke + pool_length + \n            s(days_since_start, by = swim_stroke, bs = \"gp\") + \n            s(pos_within_workout, by = swim_stroke) +\n            s(pos_within_set, by = swim_stroke),\n    data     = final_tbl_2020,\n    discrete = TRUE,\n    family   = scat(),\n    method   = \"fREML\"\n)\n\n# Model 3: Add session random effects\nset_location_re &lt;- bam(\n    speed ~ swim_stroke + pool_length + \n            s(days_since_start, by = swim_stroke, bs = \"gp\") +\n            s(pos_within_workout, by = swim_stroke) +\n            s(pos_within_set, by = swim_stroke) +\n            s(session_id, bs = \"re\"),  # Random effect for sessions\n    data     = final_tbl_2020,\n    discrete = TRUE,\n    family   = scat(),\n    method   = \"fREML\"\n)\n\n# Compare model performance\nmodel_comparison &lt;- compare_performance(simple, set_location, set_location_re)\nprint(model_comparison)\n\n# Comparison of Model Performance Indices\n\nName            | Model |    AIC (weights) |   AICc (weights)\n-------------------------------------------------------------\nsimple          |   bam | -60982.5 (&lt;.001) | -60982.4 (&lt;.001)\nset_location    |   bam | -78903.1 (&lt;.001) | -78902.5 (&lt;.001)\nset_location_re |   bam | -81777.9 (&gt;.999) | -81765.5 (&gt;.999)\n\nName            |    BIC (weights) |    R2 |  RMSE | Sigma\n----------------------------------------------------------\nsimple          | -60631.1 (&lt;.001) | 0.204 | 0.123 | 1.000\nset_location    | -78017.1 (&gt;.999) | 0.449 | 0.101 | 1.000\nset_location_re | -77560.3 (&lt;.001) | 0.475 | 0.099 | 1.000\n\nmodel_summary&lt;- summary(set_location_re)\nprint(model_summary)\n\n\nFamily: Scaled t(3.729,0.064) \nLink function: identity \n\nFormula:\nspeed ~ swim_stroke + pool_length + s(days_since_start, by = swim_stroke, \n    bs = \"gp\") + s(pos_within_workout, by = swim_stroke) + s(pos_within_set, \n    by = swim_stroke) + s(session_id, bs = \"re\")\n\nParametric coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              0.8914292  0.0049522  180.01   &lt;2e-16 ***\nswim_strokebreaststroke  0.0613416  0.0052787   11.62   &lt;2e-16 ***\nswim_strokebutterfly     0.2525302  0.0047424   53.25   &lt;2e-16 ***\nswim_strokefreestyle     0.2141422  0.0031635   67.69   &lt;2e-16 ***\npool_length             -0.0017971  0.0001487  -12.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                                                  edf  Ref.df        F  p-value\ns(days_since_start):swim_strokebackstroke       6.088   6.963    7.448  &lt; 2e-16\ns(days_since_start):swim_strokebreaststroke     5.747   6.583    4.031 0.000274\ns(days_since_start):swim_strokebutterfly        7.962   8.641    7.520  &lt; 2e-16\ns(days_since_start):swim_strokefreestyle        8.047   8.390    7.933  &lt; 2e-16\ns(pos_within_workout):swim_strokebackstroke     2.495   3.100    4.059 0.006375\ns(pos_within_workout):swim_strokebreaststroke   7.311   8.292   20.984  &lt; 2e-16\ns(pos_within_workout):swim_strokebutterfly      6.725   7.816   12.175  &lt; 2e-16\ns(pos_within_workout):swim_strokefreestyle      8.375   8.888  312.136  &lt; 2e-16\ns(pos_within_set):swim_strokebackstroke         8.232   8.778  163.531  &lt; 2e-16\ns(pos_within_set):swim_strokebreaststroke       8.347   8.804  514.321  &lt; 2e-16\ns(pos_within_set):swim_strokebutterfly          8.474   8.888  500.872  &lt; 2e-16\ns(pos_within_set):swim_strokefreestyle          8.838   8.990 1051.382  &lt; 2e-16\ns(session_id)                                 390.899 449.000    8.486  &lt; 2e-16\n                                                 \ns(days_since_start):swim_strokebackstroke     ***\ns(days_since_start):swim_strokebreaststroke   ***\ns(days_since_start):swim_strokebutterfly      ***\ns(days_since_start):swim_strokefreestyle      ***\ns(pos_within_workout):swim_strokebackstroke   ** \ns(pos_within_workout):swim_strokebreaststroke ***\ns(pos_within_workout):swim_strokebutterfly    ***\ns(pos_within_workout):swim_strokefreestyle    ***\ns(pos_within_set):swim_strokebackstroke       ***\ns(pos_within_set):swim_strokebreaststroke     ***\ns(pos_within_set):swim_strokebutterfly        ***\ns(pos_within_set):swim_strokefreestyle        ***\ns(session_id)                                 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.475   Deviance explained = 42.7%\nfREML =  65732  Scale est. = 1         n = 39572\n\nappraise(set_location_re)\n\n\n\n\n\n\n\n\nBased on model comparison, the most complex one seems to be doing the best.  Some sanity check: indeed, backstroke is my slowest stroke, whereas butterfly is the fastest, and swimming in a 50 m pool vs. a 25-yard pool slows me a bit. In terms of fixed effects, the model effectively captured them.  Smooth terms revealed significant nonlinear trends over time for all strokes, even when accounted for lap temporal position and session as a random factor. So let’s have a look. \n\ngratia::draw(set_location_re,select=1:4 )\n\n\n\n\n\n\n\n\nI am getting faster with my strokes, but I am plateauing with my freestyle. Let’s create predictions from our best model and overlay them on the actual data: \n\n# Create prediction dataset\npred_data &lt;- expand.grid(\n  days_since_start = seq(min(final_tbl_2020$days_since_start), \n                        max(final_tbl_2020$days_since_start), \n                        length.out = 200),\n  swim_stroke = levels(final_tbl_2020$swim_stroke),\n  stringsAsFactors = FALSE\n)\n\n# Set other variables to median/reference values\npred_data$pool_length &lt;- min(final_tbl_2020$pool_length) #most of my workouts are in 25 yards\npred_data$pos_within_workout &lt;- median(final_tbl_2020$pos_within_workout)\npred_data$pos_within_set &lt;- median(final_tbl_2020$pos_within_set)\npred_data$session_id &lt;- levels(final_tbl_2020$session_id)[1]\n\n# Generate predictions (excluding random effects)\npredictions &lt;- predict(set_location_re, \n                      newdata = pred_data, \n                      exclude = \"s(session_id)\",\n                      se.fit = TRUE)\n\npred_data$fit &lt;- predictions$fit\npred_data$se &lt;- predictions$se.fit\n# Validation plot\np_validation &lt;- ggplot() +\n  # Scatter with very low alpha for context\n  geom_point(data = final_tbl_2020, \n             aes(x = days_since_start, y = speed),\n             alpha = 0.05, size = 0.5, color = \"grey50\") +\n  # 2D density contour lines\n  geom_density_2d(data = final_tbl_2020, \n                  aes(x = days_since_start, y = speed),\n                  color = \"darkblue\", bins = 10, size = 0.5) +\n  # Model smooth lines only - no CI\n  geom_line(data = pred_data,\n            aes(x = days_since_start, y = fit),\n            color = \"red\", size = 1.5) +\n  # Facet by stroke type\n  facet_wrap(~ swim_stroke, scales = \"free_y\", ncol = 2) +\n  # Labels and theme\n  labs(x = \"Days Since Start\",\n       y = \"Speed (m/s)\",\n       title = \"Swimming Speed Over Time by Stroke Type\",\n       subtitle = \"Density contours with GAM smooth curves\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        panel.grid.minor = element_blank(),\n        strip.text = element_text(size = 12, face = \"bold\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nprint(p_validation)\n\n\n\n\n\n\n\n\nAlthough structured test sets would be ideal for tracking progress, even this messy lap-level data can already provide some insights. Overall, the model indicates that my backstroke, breaststroke, and butterfly are improving (which I have noticed myself), while freestyle still lags behind. Extending the analysis to quartiles or deciles of lap speeds should provide a more comprehensive picture of training progress and pacing strategies."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hello and welcome! I’m Julijus  Bogomolovas, a molecular cardiology researcher at UC San Diego who spends most days studying molecular signaling that keeps our hearts beating — but I unwind by pointing those same analytical instincts at whatever catches my curiosity, from swimming training to cherry blossom prediction.\n Think of this space as my digital lab notebook crossed with a Sunday afternoon hobby shed: you’ll find expanded write-ups of data analysis from my professional work, side quests where I teach myself new statistical tricks, and plenty of “vanity” explorations that look fun. \nI’m self‑taught in much of the data‑science tooling you’ll see here, so expect the occasional misstep, frank reflections on what I learned, and code and data you’re free to borrow or improve.\n If you’re into biostatistics, all things cardiac, R notebooks that actually run, or enjoy watching a scientist learn in public, pull up a chair and join the conversation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julius’s Blog",
    "section": "",
    "text": "Cherry Bloom Prediction\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\nJulijus Bogomolovas\n\n\n\n\n\n\n\n\n\n\n\n\nSwimming speed Analysis With Generalized Additive Models\n\n\n\nR\n\nGAM\n\nSports Analytics\n\nData Visualization\n\n\n\n\n\n\n\n\n\nJun 23, 2025\n\n\nJulius Bogomolovas\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 20, 2025\n\n\nJulius Bogomolovas\n\n\n\n\n\nNo matching items"
  }
]